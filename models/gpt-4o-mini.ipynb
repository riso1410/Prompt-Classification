{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "import dspy\n",
    "import pandas as pd\n",
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n",
    "from dspy.evaluate import Evaluate\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "proxy_url = os.getenv(\"PROXY_URL\")\n",
    "model = \"gpt-4o-mini\"\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"API key not found. Please check your environment variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.LM(\n",
    "    api_key=api_key,\n",
    "    model=model,\n",
    "    api_base=proxy_url,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_example(row: pd.Series) -> dspy.Example:\n",
    "    return dspy.Example(\n",
    "        prompt=row[\"question\"],\n",
    "        completion=row[\"answer\"],\n",
    "        label=row[\"label\"],\n",
    "    ).with_inputs(\"prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_data = pd.read_csv(\"../data/open_domain_data.csv\")\n",
    "specific_data = pd.read_csv(\"../data/specific_domain_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_examples = list()\n",
    "specific_examples = list()  \n",
    "\n",
    "for _, row in open_data.iterrows():\n",
    "    example = create_example(row=row)\n",
    "    open_examples.append(example)\n",
    "\n",
    "for _, row in specific_data.iterrows():\n",
    "    example = create_example(row=row)\n",
    "    specific_examples.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = open_examples + specific_examples\n",
    "random.shuffle(final_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signature & Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClasificationSignature(dspy.Signature):\n",
    "    \"\"\"Classify if a text is specific for a domain or not. Target domain is law.\"\"\"\n",
    "\n",
    "    prompt = dspy.InputField(desc=\"The prompt to classify.\")\n",
    "\n",
    "    explanation = dspy.OutputField(desc=\"Reasoning behind the classification.\")\n",
    "    label = dspy.OutputField(desc=\"True, if the input text is domain specific, False otherwise.\")\n",
    "    \n",
    "\n",
    "class ClassificationModule(dspy.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.prog = dspy.ChainOfThought(ClasificationSignature)\n",
    "        \n",
    "    def forward(self, prompt: str) -> ClasificationSignature:\n",
    "        prediction = self.prog(prompt=prompt)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric & Teleprompter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_answer(answer: str) -> bool:\n",
    "    answer = answer.lower()\n",
    "    if \"yes\" in answer or \"true\" in answer:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison_metric(gold, pred, trace=None) -> bool:\n",
    "    return parse_answer(gold) == parse_answer(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to sample between 1 and 2 traces per predictor.\n",
      "Will attempt to bootstrap 8 candidate sets.\n"
     ]
    }
   ],
   "source": [
    "fewshot_optimizer = BootstrapFewShotWithRandomSearch(metric=comparison_metric, max_bootstrapped_demos=2, num_candidate_programs=8, num_threads=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ClassificationModule()(prompt=open_examples[0].prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(output.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
