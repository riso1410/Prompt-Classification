{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import dspy\n",
    "import dotenv\n",
    "import random\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 22\n",
    "TRAIN_SIZE = 100\n",
    "TEST_SIZE = 1000\n",
    "TEMPERATURE = 0.2\n",
    "PRICE_PER_1_000_000_TOKENS = 0.15 \n",
    "PRICE_PER_1_000_000_TOKENS_OUTPUT = 0.60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "proxy_url = os.getenv(\"PROXY_URL\")\n",
    "model = \"gpt-4o-mini\"\n",
    "\n",
    "encoder = tiktoken.encoding_for_model(model)\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"API key not found. Please check your environment variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.LM(\n",
    "    api_key=api_key,\n",
    "    model=model,\n",
    "    api_base=proxy_url,\n",
    "    temperature=TEMPERATURE,\n",
    ")\n",
    "\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_example(row: pd.Series) -> dspy.Example:\n",
    "    return dspy.Example(\n",
    "        prompt=row[\"question\"],\n",
    "        completion=row[\"answer\"],\n",
    "        label=row[\"label\"],\n",
    "    ).with_inputs(\"prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_data = pd.read_csv(\"../data/open_domain_data.csv\")\n",
    "specific_data = pd.read_csv(\"../data/specific_domain_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_examples = list()\n",
    "specific_examples = list()  \n",
    "\n",
    "for _, row in open_data.iterrows():\n",
    "    example = create_example(row=row)\n",
    "    open_examples.append(example)\n",
    "\n",
    "for _, row in specific_data.iterrows():\n",
    "    example = create_example(row=row)\n",
    "    specific_examples.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = open_examples + specific_examples\n",
    "random.shuffle(final_data)\n",
    "\n",
    "train_data = final_data.sample(n=TRAIN_SIZE, random_state=SEED)\n",
    "test_data = final_data.sample(n=TEST_SIZE, random_state=SEED)\n",
    "\n",
    "print(f\"Train data: {len(train_data)}\")\n",
    "print(f\"Test data: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signature & Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClasificationSignature(dspy.Signature):\n",
    "    \"\"\"Classify if a text is specific for a domain or not. Target domain is law.\"\"\"\n",
    "\n",
    "    prompt = dspy.InputField(desc=\"The prompt to classify.\")\n",
    "\n",
    "    #explanation = dspy.OutputField(desc=\"Reasoning behind the classification.\")\n",
    "    label = dspy.OutputField(desc=\"1, if the input text is law domain, 0 otherwise.\")\n",
    "    \n",
    "\n",
    "class ClassificationModule(dspy.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.prog = dspy.ChainOfThought(ClasificationSignature)\n",
    "        \n",
    "    def forward(self, prompt: str) -> ClasificationSignature:\n",
    "        prediction = self.prog(prompt=prompt)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric & Teleprompter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_answer(answer) -> bool:\n",
    "    if isinstance(answer, str) and re.match(r\"^[01]$\", answer.strip()):\n",
    "        return bool(int(answer))\n",
    "    elif isinstance(answer, int) and answer in [0, 1]:\n",
    "        return bool(answer)\n",
    "    else:\n",
    "        print(f\"Unexpected non-binary label found: {answer}\")\n",
    "        return False\n",
    "\n",
    "def evaluate_model(predictions, true_labels):\n",
    "    parsed_preds = [parse_answer(pred) for pred in predictions]\n",
    "    parsed_labels = [parse_answer(label) for label in true_labels]\n",
    "    \n",
    "    accuracy = accuracy_score(parsed_labels, parsed_preds)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison_metric(example, pred, trace=None) -> bool:\n",
    "    return parse_answer(example.label) == parse_answer(pred.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fewshot_optimizer = BootstrapFewShotWithRandomSearch(\n",
    "    metric=comparison_metric,\n",
    "    max_bootstrapped_demos = 4,\n",
    "    max_labeled_demos = 5,\n",
    "    max_rounds = 1,\n",
    "    num_candidate_programs = 5,\n",
    ")\n",
    "\n",
    "# Optimize the model\n",
    "compiled_classification = fewshot_optimizer.compile(ClassificationModule(), trainset=train_data)\n",
    "\n",
    "# Save the model\n",
    "compiled_classification.save(\"classification_model.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Price calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(prompt: str) -> int:\n",
    "    \"\"\"Count tokens in the given text.\"\"\"\n",
    "    tokens = encoder.encode(prompt)\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_price(token_count: int) -> float:\n",
    "    \"\"\"Calculate the price based on token count.\"\"\"\n",
    "    return (token_count / 1_000_000) * PRICE_PER_1_000_000_TOKENS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = list()\n",
    "true_labels = list()\n",
    "\n",
    "total_tokens = int()\n",
    "\n",
    "for example in test_data[:TEST_SIZE]:  \n",
    "    prompt = example.prompt    \n",
    "    output = compiled_classification(prompt)\n",
    "    \n",
    "    total_tokens += count_tokens(prompt)\n",
    "    total_tokens += count_tokens(output.label)\n",
    "    \n",
    "    predictions.append(output.label)\n",
    "    true_labels.append(example.label)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation_results = evaluate_model(predictions, true_labels)\n",
    "\n",
    "print(f\"Evaluation results: Accuracy {evaluation_results * 100}%\")\n",
    "print(f\"Total price: {calculate_price(total_tokens)} USD\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
