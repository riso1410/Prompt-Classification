{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\riso\\Desktop\\Prompt-Classification\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gpt_4o_mini import Trainer, GPT4Model\n",
    "from svm_tfidf import SVMClassifier\n",
    "import os\n",
    "import dotenv\n",
    "import requests\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['gpt-4o', 'llama3.1:70b', 'llama3.1:8b', 'o1-mini', 'o1-preview', 'gpt-4', 'gpt-4o-mini', 'gpt-4-turbo']\n"
     ]
    }
   ],
   "source": [
    "def get_models() -> list:\n",
    "    url = f\"{os.getenv(\"PROXY_URL\")}/models\"\n",
    "    headers = {\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {os.getenv(\"OPENAI_API_KEY\")}\"}\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        models = response.json()\n",
    "        models = [model[\"id\"] for model in models[\"data\"]]\n",
    "        return models\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "available_models = get_models()\n",
    "if available_models:\n",
    "    print(\"Available models:\", available_models)\n",
    "else:\n",
    "    print(\"Failed to retrieve models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 10\n",
      "Test data: 20\n",
      "Going to sample between 1 and 4 traces per predictor.\n",
      "Will attempt to bootstrap 5 candidate sets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 5 / 10  (50.0): 100%|██████████| 10/10 [00:07<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 50.0 for seed -3\n",
      "Scores so far: [50.0]\n",
      "Best score so far: 50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 10 / 10  (100.0): 100%|██████████| 10/10 [00:06<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 100.0 for seed -2\n",
      "Scores so far: [50.0, 100.0]\n",
      "Best score so far: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:03<00:05,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 10 / 10  (100.0): 100%|██████████| 10/10 [00:04<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [50.0, 100.0, 100.0]\n",
      "Best score so far: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:07<00:11,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 10 / 10  (100.0): 100%|██████████| 10/10 [00:03<00:00,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [50.0, 100.0, 100.0, 100.0]\n",
      "Best score so far: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:03<00:14,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 3 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 10 / 10  (100.0): 100%|██████████| 10/10 [00:03<00:00,  2.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [50.0, 100.0, 100.0, 100.0, 100.0]\n",
      "Best score so far: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:01<00:14,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 10 / 10  (100.0): 100%|██████████| 10/10 [00:02<00:00,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [50.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
      "Best score so far: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:03<00:13,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 3 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 10 / 10  (100.0): 100%|██████████| 10/10 [00:05<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [50.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
      "Best score so far: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:03<00:12,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 3 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 10 / 10  (100.0): 100%|██████████| 10/10 [00:05<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [50.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
      "Best score so far: 100.0\n",
      "8 candidate programs found.\n",
      "Evaluation results: Accuracy 95.00%\n"
     ]
    }
   ],
   "source": [
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "proxy_url = os.getenv(\"PROXY_URL\")\n",
    "\n",
    "\n",
    "gpt_model = GPT4Model(api_key=api_key, proxy_url=proxy_url, domain='law', model_name='gpt-4o-mini', train_size=10, test_size=20)\n",
    "gpt_model.load_data(\"../data/open_domain_data.csv\", \"../data/specific_domain_data.csv\")\n",
    "\n",
    "trainer = Trainer(gpt_model.train_data)\n",
    "compiled_model = trainer.optimize_model()\n",
    "accuracy = trainer.test_model(gpt_model.test_data)\n",
    "\n",
    "print(f\"Evaluation results: Accuracy {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('prog', Predict(StringSignature(prompt -> reasoning, target, label\n",
      "    instructions='Classify if a text is specific for a domain or not.'\n",
      "    prompt = Field(annotation=str required=True json_schema_extra={'desc': 'The prompt to classify.', '__dspy_field_type': 'input', 'prefix': 'Prompt:'})\n",
      "    reasoning = Field(annotation=str required=True json_schema_extra={'prefix': \"Reasoning: Let's think step by step in order to\", 'desc': '${reasoning}', '__dspy_field_type': 'output'})\n",
      "    target = Field(annotation=str required=True json_schema_extra={'desc': 'The target domain to classify the prompt against.', '__dspy_field_type': 'output', 'prefix': 'Target:'})\n",
      "    label = Field(annotation=str required=True json_schema_extra={'desc': '1, if the input text belong to domain, 0 otherwise.', '__dspy_field_type': 'output', 'prefix': 'Label:'})\n",
      ")))]\n",
      "Model saved to ../models/gpt-4o-mini.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(f\"../models/{gpt_model.model_name}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM-TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier = SVMClassifier(C=0.1, train_size=10_000, test_size=10_000, seed=22)\n",
    "\n",
    "# Prepare data (assuming 'dummy_open.csv' and 'dummy_specific.csv' are paths to your data files)\n",
    "X_train, X_test, y_train, y_test = svm_classifier.prepare_data(\"../data/open_domain_data.csv\", \"../data/specific_domain_data.csv\")\n",
    "\n",
    "# Train the model\n",
    "svm_classifier.train(X_train, y_train)\n",
    "\n",
    "# Cross-validate\n",
    "cv_results = svm_classifier.cross_validate_model(X_train, y_train, cv=3)\n",
    "print(\"Cross-validation Results:\", cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test data\n",
    "evaluation_results = svm_classifier.evaluate(X_test, y_test)\n",
    "print(\"Evaluation Results:\", evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "svm_classifier.save_model('svm_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
