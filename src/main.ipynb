{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\riso\\Desktop\\Prompt-Classification\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gpt_4o_mini import *\n",
    "from svm_tfidf import DataPreparationSVM, SVMClassifier\n",
    "from utilities import Config\n",
    "import os\n",
    "import dotenv\n",
    "import requests\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['gpt-4o-mini', 'gpt-4', 'gpt-4o', 'llama3.1:70b', 'o1-mini', 'llama3.1:8b', 'o1-preview', 'gpt-4-turbo']\n"
     ]
    }
   ],
   "source": [
    "def get_models() -> list:\n",
    "    url = f\"{os.getenv(\"PROXY_URL\")}/models\"\n",
    "    headers = {\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {os.getenv(\"OPENAI_API_KEY\")}\"}\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        models = response.json()\n",
    "        models = [model[\"id\"] for model in models[\"data\"]]\n",
    "        return models\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "available_models = get_models()\n",
    "if available_models:\n",
    "    print(\"Available models:\", available_models)\n",
    "else:\n",
    "    print(\"Failed to retrieve models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Train data: 20\n",
      "Test data: 20\n",
      "Going to sample between 1 and 4 traces per predictor.\n",
      "Will attempt to bootstrap 5 candidate sets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 19 / 20  (95.0): 100%|██████████| 20/20 [00:00<00:00, 413.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 95.0 for seed -3\n",
      "Scores so far: [95.0]\n",
      "Best score so far: 95.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 19 / 20  (95.0): 100%|██████████| 20/20 [00:00<00:00, 484.49it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [95.0, 95.0]\n",
      "Best score so far: 95.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 19 / 20  (95.0): 100%|██████████| 20/20 [00:00<00:00, 534.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [95.0, 95.0, 95.0]\n",
      "Best score so far: 95.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 19 / 20  (95.0): 100%|██████████| 20/20 [00:00<00:00, 487.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [95.0, 95.0, 95.0, 95.0]\n",
      "Best score so far: 95.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 3 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 19 / 20  (95.0): 100%|██████████| 20/20 [00:00<00:00, 405.69it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [95.0, 95.0, 95.0, 95.0, 95.0]\n",
      "Best score so far: 95.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 19 / 20  (95.0): 100%|██████████| 20/20 [00:00<00:00, 868.72it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [95.0, 95.0, 95.0, 95.0, 95.0, 95.0]\n",
      "Best score so far: 95.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 3 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 19 / 20  (95.0): 100%|██████████| 20/20 [00:00<00:00, 810.05it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [95.0, 95.0, 95.0, 95.0, 95.0, 95.0, 95.0]\n",
      "Best score so far: 95.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 3 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 19 / 20  (95.0): 100%|██████████| 20/20 [00:00<00:00, 817.30it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [95.0, 95.0, 95.0, 95.0, 95.0, 95.0, 95.0, 95.0]\n",
      "Best score so far: 95.0\n",
      "8 candidate programs found.\n",
      "Evaluation results: Accuracy 95.00%\n"
     ]
    }
   ],
   "source": [
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "proxy_url = os.getenv(\"PROXY_URL\")\n",
    "\n",
    "config = Config(train_size=20, test_size=20)\n",
    "data_prep = DataPreparationLLM(config)\n",
    "data_prep.load_data(\"../data/open_domain_data.csv\", \"../data/specific_domain_data.csv\")\n",
    "\n",
    "gpt_model = GPT4Model(api_key=api_key, proxy_url=proxy_url, model_name='gpt-4o-mini')\n",
    "evaluator = Evaluator()\n",
    "\n",
    "trainer = Trainer(ClassificationModule, data_prep.train_data, evaluator)\n",
    "compiled_model = trainer.optimize_model()\n",
    "\n",
    "predictions = list()\n",
    "true_labels = [example.label for example in data_prep.test_data]\n",
    "\n",
    "total_tokens = int()\n",
    "\n",
    "for example in data_prep.test_data:\n",
    "    prompt = example.prompt\n",
    "    prediction = compiled_model(prompt)\n",
    "    predictions.append(prediction.label)\n",
    "\n",
    "accuracy = evaluator.evaluate_model(predictions, true_labels)\n",
    "\n",
    "print(f\"Evaluation results: Accuracy {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('prog', Predict(StringSignature(prompt -> reasoning, label\n",
      "    instructions='Classify if a text is specific for a domain or not. Target domain is law.'\n",
      "    prompt = Field(annotation=str required=True json_schema_extra={'desc': 'The prompt to classify.', '__dspy_field_type': 'input', 'prefix': 'Prompt:'})\n",
      "    reasoning = Field(annotation=str required=True json_schema_extra={'prefix': \"Reasoning: Let's think step by step in order to\", 'desc': '${reasoning}', '__dspy_field_type': 'output'})\n",
      "    label = Field(annotation=str required=True json_schema_extra={'desc': '1, if the input text is law domain, 0 otherwise.', '__dspy_field_type': 'output', 'prefix': 'Label:'})\n",
      ")))]\n",
      "Model saved to ../models/gpt-4o-mini.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(f\"../models/{gpt_model.model_name}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM-TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Train size: 20000\n",
      "Test size: 10000\n"
     ]
    }
   ],
   "source": [
    "config = Config(train_size=20_000, test_size=10_000, seed=4444)\n",
    "\n",
    "data_prep = DataPreparationSVM(config)\n",
    "data = data_prep.load_data(\"../data/open_domain_data.csv\", \"../data/specific_domain_data.csv\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = data_prep.prepare_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([17.03054047, 16.99490213, 17.06809402, 17.08671927, 24.49330306]), 'score_time': array([4.01497626, 3.96303582, 3.97708201, 4.43841743, 4.03846121]), 'test_accuracy': array([0.959  , 0.95475, 0.951  , 0.954  , 0.964  ]), 'test_precision': array([0.941247  , 0.93645084, 0.93273273, 0.93686109, 0.9510574 ]), 'test_recall': array([0.9596577 , 0.95418448, 0.94868662, 0.95174099, 0.96151497])}\n",
      "Accuracy: 96.06%\n",
      "Precision: 94.81%\n",
      "Recall: 95.47%\n"
     ]
    }
   ],
   "source": [
    "svm_classifier = SVMClassifier(config)\n",
    "cross_val_scores = svm_classifier.cross_validate_model(X_train, y_train, cv=5)\n",
    "print(cross_val_scores)\n",
    "svm_classifier.train(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier \n",
    "accuracy, precision, recall = svm_classifier.evaluate(X_test, y_test)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Precision: {precision * 100:.2f}%\")\n",
    "print(f\"Recall: {recall * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 96.84%\n",
      "Training Precision: 95.77%\n",
      "Training Recall: 96.54%\n"
     ]
    }
   ],
   "source": [
    "train_accuracy, train_precision, train_recall = svm_classifier.evaluate(X_train, y_train)\n",
    "print(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Training Precision: {train_precision * 100:.2f}%\")\n",
    "print(f\"Training Recall: {train_recall * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ../models/SVM_TFIDF.joblib\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "svm_classifier.save_model(f\"../models/{svm_classifier.model_name}.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM + CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
